{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discopy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define neural networks\n",
    "\n",
    "We start by defining neurons, layers and neural nets as composites of scalar multiplication, bias, copy and merge. See arxiv:1711.10455."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discopy.function import *\n",
    "\n",
    "def mult(weight): \n",
    "    return Function('mult({})'.format(str(weight)), 1, 1, lambda x: weight * x)\n",
    "    \n",
    "def mults(dom, weights):\n",
    "    result = Id(0)\n",
    "    for i in range(dom):\n",
    "        result = result @ mult(weights[i])\n",
    "    result._name = 'mults({})'.format(str(weights))\n",
    "    return result\n",
    "\n",
    "def bias(weight):\n",
    "    return Function('bias({})'.format(str(weight)), Dim(0), Dim(1), lambda x: np.array([weight]))\n",
    "\n",
    "def merge(cod, copies):\n",
    "    @discofunc(cod * copies, cod, name='merge({}, {})'.format(cod, copies))\n",
    "    def add(x):\n",
    "        return np.array([np.sum([x[i + cod * j] for j in range(copies)]) for i in range(cod)])\n",
    "    return add\n",
    "\n",
    "sigmoid = Function('sigmoid', 1, 1, lambda x: 1/(1 + np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron(dom, weights, beta=0): # weights is a 1d array of length dom, beta is a scalar bias\n",
    "    return mults(dom, weights) @ bias(beta) >> merge(1, dom + 1) >> sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can jit and take the gradient of neurons using Jax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7875132]\n",
      "[0.         0.35140604 0.05020086 0.01673362]\n"
     ]
    }
   ],
   "source": [
    "from jax import jit, grad\n",
    "\n",
    "print(jit(neuron(4, [0, 2.1, 0.3, 0.1]))(np.array([0., 0.3, 1.2, 3.2])))\n",
    "print(grad(lambda x: neuron(4, [0., 2.1, 0.3, 0.1])(x)[0])(np.array([0., 0.3, 1.2, 3.2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(dom, cod, weights, biases): # weights is an array of shape: (cod, dom) (note cod = number of neurons)\n",
    "    neurons = Id(0)                   # biases is a 1d array of shape (cod, )\n",
    "    for i in range(cod):\n",
    "        neurons = neurons @ neuron(dom, weights[i], biases[i])\n",
    "    return Copy(dom, cod) >> neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "disconnected_layer = lambda x: layer(3, 1, [[0., 0., 0.]], [0., 0.])(x)[0]\n",
    "assert np.all(grad(disconnected_layer)(np.array([2., 3.4, 1.])) == np.array([0., 0., 0.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net(layers, weights, biases): # each layer has layer[i] neurons and is fully connected to adjacent layers \n",
    "    # weights is a list of arrays of shape (layers[i+1], layers[i]) of length len(layers) - 1\n",
    "    # biases is a list of 1d arrays of shape (layers[i], ) of length len(layers) - 1\n",
    "    result = Id(layers[0])\n",
    "    for i in range(len(layers) - 1):\n",
    "        result = result >> layer(layers[i], layers[i + 1], weights[i], biases[i])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74754167\n",
      "[DeviceArray(-0.0095527, dtype=float32), DeviceArray(0.01538139, dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "from random import uniform\n",
    "\n",
    "layers = [2, 3, 1]\n",
    "weights = [[[uniform(-10, 10) for k in range(layers[i])] for j in range(layers[i+1])] for i in range(len(layers) - 1)]\n",
    "biases = [[uniform(-5, 5) for j in range(layers[i+1])] for i in range(len(layers) - 1)]\n",
    "\n",
    "nnet = lambda x: neural_net(layers, weights, biases)(x)[0]\n",
    "jit(nnet)\n",
    "print(nnet([2.23, 4.2]))\n",
    "print(grad(nnet)([2., 4.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a language of grammatical sentences\n",
    "\n",
    "Now we fix a vocabulary and generate grammatical sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discopy.pivotal import Ty, Box, Diagram\n",
    "from discopy.pregroup import Word\n",
    "\n",
    "s, n = Ty('s'), Ty('n')\n",
    "Alice = Word('Alice', n)\n",
    "loves = Word('loves', n.r @ s @ n.l)\n",
    "Bob =  Word('Bob', n)\n",
    "who = Word('who', n.r @ n @ s.l @ n)\n",
    "is_rich = Word('is rich', n.r @ s)\n",
    "\n",
    "vocab = [Alice, who, is_rich, loves, Bob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brute force search for grammatical sentences:\n",
      "Alice is rich.\n",
      "Bob is rich.\n",
      "Alice loves Alice.\n",
      "Alice loves Bob.\n",
      "Bob loves Alice.\n",
      "Bob loves Bob.\n",
      "Alice who is rich is rich.\n",
      "Bob who is rich is rich.\n",
      "Alice who is rich loves Alice.\n",
      "Alice who is rich loves Bob.\n",
      "Alice who loves Alice is rich.\n",
      "Alice who loves Bob is rich.\n",
      "Bob who is rich loves Alice.\n",
      "Bob who is rich loves Bob.\n",
      "Bob who loves Alice is rich.\n",
      "Bob who loves Bob is rich.\n",
      "Alice who who is rich is rich is rich.\n",
      "Alice who is rich who is rich is rich.\n",
      "Alice who loves Alice loves Alice.\n",
      "Alice who loves Alice loves Bob.\n",
      "11.90 seconds to generate 20 sentences.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from discopy.pregroup import brute_force\n",
    "\n",
    "gen, n_sentences = brute_force(*vocab), 20\n",
    "sentences, parsing = list(), dict()\n",
    "\n",
    "print(\"Brute force search for grammatical sentences:\")\n",
    "\n",
    "start = time()\n",
    "for i in range(n_sentences):\n",
    "    diagram = next(gen)\n",
    "    sentence = ' '.join(str(w)\n",
    "        for w in diagram.boxes if isinstance(w, Word)) + '.'\n",
    "    sentences.append(sentence)\n",
    "    parsing.update({sentence: diagram})\n",
    "    print(sentence)\n",
    "\n",
    "print(\"{:.2f} seconds to generate {} sentences.\\n\".format(time() - start, n_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autonomization\n",
    "\n",
    "In order to map grammatical sentences to neural networks we need to turn the parsed sentences into diagrams that do not contain cups or caps. This procedure is known as autonomization, see arxiv:1411.3827 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discopy.pivotal import Cup, Cap, PivotalFunctor\n",
    "\n",
    "love_box = Box('love_box', n @ n, s)\n",
    "is_rich_box = Box('is_rich_box', n, s)\n",
    "who_box0 = Box('who_box0', n, n @ n)\n",
    "who_box1 = Box('who_box1', n @ s, n)\n",
    "\n",
    "ob = {n: n, s: s}\n",
    "ar = {Alice: Alice,\n",
    "      Bob: Bob,\n",
    "      loves: Cap(n.r, n) @ Cap(n, n.l) >> Diagram.id(n.r) @ love_box @ Diagram.id(n.l),\n",
    "      is_rich: Cap(n.r, n) >> Diagram.id(n.r) @ is_rich_box,\n",
    "      who: Cap(n.r, n) >> Diagram.id(n.r) @ (who_box0 >> Diagram.id(n) @ Cap(s, s.l) @ Diagram.id(n) >>\n",
    "                                             who_box1 @ Diagram.id(s.l @ n))\n",
    "     }\n",
    "\n",
    "A = PivotalFunctor(ob, ar, ob_cls=Ty, ar_cls=Diagram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "autonomised_parsing = {sentences[i]: A(parsing[sentences[i]]).normal_form() for i in range(n_sentences)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum model generates the distribution to be learned\n",
    "\n",
    "We use a quantum CircuitModel to generate a corpus of true sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discopy import CircuitModel\n",
    "from discopy.circuit import Circuit, sqrt, Ket, H, Rx, CX, SWAP\n",
    "\n",
    "GHZ = sqrt(2) @ Ket(0, 0, 0)\\\n",
    "    >> Circuit.id(1) @ H @ Circuit.id(1)\\\n",
    "    >> Circuit.id(1) @ CX\\\n",
    "    >> (SWAP >>  CX) @ Circuit.id(1)\n",
    "\n",
    "def intransitive_ansatz(phase):\n",
    "    return Ket(0) >> Rx(phase)\n",
    "\n",
    "def transitive_ansatz(phase):\n",
    "    return sqrt(2) @ Ket(0, 0) >> H @ Rx(phase) >> CX\n",
    "\n",
    "ob_q = {s: 0, n: 1}\n",
    "ar_q = lambda params: {\n",
    "    Alice: Ket(0),\n",
    "    loves: transitive_ansatz(params['loves']),\n",
    "    Bob: Ket(1),\n",
    "    who: GHZ,\n",
    "    is_rich: intransitive_ansatz(params['is_rich'])}\n",
    "\n",
    "QModel = CircuitModel(ob_q, ar_q({'loves': 0.5, 'is_rich': 1.}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus = {sentence: QModel(parsing[sentence]).measure() for sentence in sentences}\n",
    "\n",
    "epsilon = 1e-2\n",
    "\n",
    "# print(\"True sentences:\\n{}\\n\".format('\\n'.join(sentence\n",
    "#     for sentence, probability in corpus.items() if probability > 1 - epsilon)))\n",
    "# print(\"False sentences:\\n{}\".format('\\n'.join(sentence\n",
    "#     for sentence, probability in corpus.items() if probability < epsilon)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a neural network to reproduce the same corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sentence_train, sentence_test = train_test_split(sentences, test_size=0.5, random_state=42)\n",
    "\n",
    "# print(\"Training set:\\n{}\\n\".format('\\n'.join(sentence_train)))\n",
    "# print(\"Testing set:\\n{}\".format('\\n'.join(map(str, sentence_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function with respect to a NumpyFunctor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discopy.function import NumpyFunctor\n",
    "\n",
    "layers = {'loves': [4, 5, 1],\n",
    "          'is_rich': [2, 3, 1],\n",
    "          'who': [3, 2]}\n",
    "\n",
    "ob = {s: Dim(1), n: Dim(2)}\n",
    "\n",
    "ar = lambda params: {\n",
    "    Alice: Function('Alice', 0, 2, lambda x: np.array([1., 0.])),\n",
    "    Bob: Function('Alice', 0, 2, lambda x: np.array([0., 1.])),\n",
    "    love_box: neural_net(layers['loves'], params['loves'][0], params['loves'][1]),\n",
    "    is_rich_box: neural_net(layers['is_rich'], params['is_rich'][0], params['is_rich'][1]),\n",
    "    who_box0: Copy(2, 2),\n",
    "    who_box1: neural_net(layers['who'], params['who'][0], params['who'][1])}\n",
    "\n",
    "F = lambda params: NumpyFunctor(ob, ar({'loves': params[0], 'is_rich': params[1], 'who': params[2]}))\n",
    "\n",
    "evaluate = lambda F, sentence: F(autonomised_parsing[sentence])([])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_params(layers):\n",
    "    weights = [[[uniform(-10, 10) for k in range(layers[i])] for j in range(layers[i+1])] for i in range(len(layers) - 1)]\n",
    "    biases = [[uniform(-10, 10) for j in range(layers[i+1])] for i in range(len(layers) - 1)]\n",
    "    return [weights, biases]\n",
    "\n",
    "params0 = [rand_params(layers['loves']), rand_params(layers['is_rich']), rand_params(layers['who'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(0.5265648, dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@jit\n",
    "def training_loss(params):\n",
    "    return np.mean(np.array([\n",
    "        (corpus[sentence] - evaluate(F(params), sentence)) ** 2\n",
    "        for sentence in sentence_train]))\n",
    "\n",
    "@jit\n",
    "def testing_loss(params):\n",
    "    return np.mean(np.array([\n",
    "        (corpus[sentence] - evaluate(F(params), sentence)) ** 2\n",
    "        for sentence in sentence_test]))\n",
    "\n",
    "training_loss(params0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
